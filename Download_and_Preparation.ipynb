{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOADING DATA AND PREPARING FOR FURTHER CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import wrds\n",
    "import os\n",
    "import re\n",
    "from utils import load_data, save_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LOADING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 CONNECTING TO THE WHARTON DATABASE (Skip this step if you don't want to download the data from the database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# CONNECTING TO THE WHARTON DATABASE\n",
    "wrds_db = wrds.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a subset list of countries in EU AND US to extract from wrds, the country code is given by JPK on their repository\n",
    "countries = pd.read_excel(\"Country Classification.xlsx\")\n",
    "subset_countries = countries.query(\"region == 'north america' or region == 'europe'\").excntry.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading and extracting list of characteristics\n",
    "chars = pd.read_excel('https://github.com/bkelly-lab/ReplicationCrisis/raw/master/GlobalFactors/Factor%20Details.xlsx')\n",
    "chars_rel = chars[chars['abr_jkp'].notna()]['abr_jkp'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A STRING VERSION OF THE FACTORS FOR THE SQL QUERY\n",
    "vars = ', '.join(map(str, chars_rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 EXTRACTING THE DATA OR LOADING IT LOCALLY IF ALREADY SAVED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHANGE `GET_DATA` to True IF YOU WANT TO RE-FETCH THE DATA FROM THE DATABASE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting Date for our purposes\n",
    "date = '1995-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes Loaded: 40/40\r"
     ]
    }
   ],
   "source": [
    "# Extracting the factors\n",
    "if GET_DATA:\n",
    "        query = f\"\"\"\n",
    "                SELECT id, eom, excntry, gvkey, permno, size_grp, me, {vars}\n",
    "                        FROM contrib.global_factor\n",
    "                        WHERE common=1 \n",
    "                        and \n",
    "                        exch_main=1 \n",
    "                        and \n",
    "                        primary_sec=1 \n",
    "                        and \n",
    "                        obs_main=1\n",
    "                        and\n",
    "                        excntry = 'USA'\n",
    "                        and\n",
    "                        date > '{date}'\n",
    "                \"\"\"\n",
    "\n",
    "        data = wrds_db.raw_sql(query)\n",
    "        save_data(data)\n",
    "else:\n",
    "        df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING THE MONTHLY RETURN DATA AND THE INDUSTRY SECTOR\n",
    "if GET_DATA:\n",
    "        # Extracting the return\n",
    "        query_ret = f\"\"\"\n",
    "                SELECT id, eom, excntry, gvkey, permno, ret, ff49\n",
    "                        FROM contrib.global_factor\n",
    "                        WHERE common=1 \n",
    "                        and \n",
    "                        exch_main=1 \n",
    "                        and \n",
    "                        primary_sec=1 \n",
    "                        and \n",
    "                        obs_main=1\n",
    "                        and\n",
    "                        excntry = 'USA'\n",
    "                        and\n",
    "                        date > '{date}'\n",
    "                \"\"\"\n",
    "\n",
    "        ret = wrds_db.raw_sql(query_ret) # get data\n",
    "        ret.eom = pd.to_datetime(ret.eom)\n",
    "        ret.to_csv(\"return_GFD.csv\", index = False) # save data\n",
    "else:\n",
    "        ret = pd.read_csv(\"return_GFD.csv\")\n",
    "        ret.eom = pd.to_datetime(ret.eom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CLEANING AND MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE SMALL COMPANIES\n",
    "l_remove = [\"micro\",\"nano\",\"NAN\",\"small\"]\n",
    "df.size_grp = df.size_grp.fillna(\"NAN\")\n",
    "df = df[~df.size_grp.isin(l_remove)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unique companies</th>\n",
       "      <td>5645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unique periods</th>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  TOTAL\n",
       "Unique companies   5645\n",
       "Unique periods      348"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMOVING OBSERVATION IN 2024\n",
    "df.eom = pd.to_datetime(df.eom)\n",
    "df = df[df.eom.dt.year <= 2023].loc[:,:].reset_index(drop = True)\n",
    "n_companies = len(df.id.unique())\n",
    "n_dates = len(df.eom.unique())\n",
    "pd.DataFrame({\"Unique companies\" : [n_companies], \"Unique periods\" : [n_dates]}).T.rename(columns = {0: \"TOTAL\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is right as the number of periods from $k = 1995$  and  $n = 2023$  for  S = 12$ time periods per year, is: $T = (n - k + 1) * S = 348$  \n",
    "  \n",
    "Note that we remove the three observed months from 2024 for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately only 226 companies have data for every period\n"
     ]
    }
   ],
   "source": [
    "# RUN A CHECK\n",
    "complete_data = pd.DataFrame(df.groupby(\"id\").size()).sort_values(0,ascending = False).rename(columns = {0:\"num\"}).query(\"num == 348\").shape[0]\n",
    "print(f\"Unfortunately only {complete_data} companies have data for every period\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE THE TARGET (Return at time t) WITH THE FACTORS\n",
    "df_returns = pd.merge(df,ret[[\"id\",\"eom\",\"ret\",\"ff49\"]], how = \"left\", on = [\"id\",\"eom\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Overview\n",
    "\n",
    "1. **Sort DataFrame**: The `df_returns` DataFrame is sorted by `id` and `eom` (end of month/date). So that we can look at each company's time series sequentially\n",
    "2. **Loop through Rows**: For each row (except the last):\n",
    "- If we reach the last observation (likely 2023) for a company, the target variable takes value `None`, as we don't have the following return\n",
    "- If the return in the next row is reported more than 31 days after the current return, we set the target to `None` as again, we do not have the following return\n",
    "\n",
    "\n",
    "The result is a list `y` that stores the lead of `ret` values based on the conditions above.  \n",
    "After removing the last row from our dataset, we include `y` as a column.  \n",
    "\n",
    "Example\n",
    "\n",
    "| **id (company)**  | **eom (date)**       | **ret (time t)**  | **y (time t + 1)**|\n",
    "|:-----|:----------|-----:|---------:|\n",
    "| A    | 2024-01-31| 0.10 |      0.2 |\n",
    "| A    | 2024-03-31| 0.20 |      NaN |\n",
    "| A    | 2024-04-30| 0.15 |      NaN |\n",
    "| B    | 2024-01-31| 0.05 |      0.10 |\n",
    "| B    | 2024-02-28| 0.10 |      NaN |\n",
    "| C    | 2024-01-31| 0.25 |      0.30 |\n",
    "| C    | 2024-02-28| 0.30 |      0.35 |\n",
    "| C    | 2024-03-31| 0.35 |      NaN |\n",
    "| D    | 2024-03-05| 0.40 |      NaN |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "df_returns.sort_values([\"id\",\"eom\"], inplace = True)\n",
    "df_returns.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Loop through rows and store the value in y\n",
    "y = []\n",
    "for i in range(len(df_returns) - 1):\n",
    "    current_id = df_returns.loc[i,\"id\"]\n",
    "    current_date = df_returns.loc[i,\"eom\"]\n",
    "    next_id = df_returns.loc[i + 1,\"id\"]\n",
    "    next_date = df_returns.loc[i + 1,\"eom\"]\n",
    "\n",
    "    # If we the next row contains another company's data append None and move on\n",
    "    if current_id != next_id:\n",
    "        y.append(None)\n",
    "        continue\n",
    "    \n",
    "    # If the next row contains data for the same company less than a month apart append the return at t+1\n",
    "    if (next_date - current_date).days <= 31:\n",
    "        r = df_returns.loc[i + 1,\"ret\"]\n",
    "        y.append(r)\n",
    "    # If the next row contains data more than one month apart, append none\n",
    "    else:\n",
    "        y.append(None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 13907 observations lost\n",
      "\n",
      "Of these: 8262 are lost because the return of the following month is missing. \n",
      "The remaining 5645 are lost because the last observation for each company is not followed by any return, hence we lose one observation per company.\n"
     ]
    }
   ],
   "source": [
    "# Checks\n",
    "print(f\"There are a total of {y.count(None)} observations lost\\n\\nOf these: {y.count(None) - len(df_returns.id.unique())} are lost because the return of the following month is missing. \\nThe remaining {len(df_returns.id.unique())} are lost because the last observation for each company is not followed by any return, hence we lose one observation per company.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the target variable in the datagframe\n",
    "df_returns = df_returns.iloc[:-1]\n",
    "df_returns.loc[:,\"y\"] = y\n",
    "df_final = df_returns[~df_returns.y.isna()].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "SAVE = False\n",
    "if SAVE:\n",
    "    df_final.to_csv(\"GFD_final.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
